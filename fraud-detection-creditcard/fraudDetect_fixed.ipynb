{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=pd.read_csv('../creditcard.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.isnull()`: This is a method applied to the DataFrame that checks each element in the DataFrame and returns a DataFrame of the same shape where each element is either `True` if it's a missing value `null` or `False` if it's not.<br>\n",
    "`.sum()`: This is applied to the resulting DataFrame from the previous step. It calculates the sum of True values along each column. Since `True` is treated as `1` and `False` as `0` when summing, this effectively counts the number of True values in each column.<br>\n",
    "| V1 | V2 | V3 |\n",
    "|----------|----------|----------|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "|sum = 1|sum = 3|sum = 2|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.duplicated()`: This is a method applied to the DataFrame that checks each row and returns a Boolean Series where each element is `True` if the corresponding row is a `duplicate of a previous row`, and `False` if it's `not a duplicate`.<br>\n",
    "`.sum()`:Is same as before<br>\n",
    "| V1 | V2 | V3 | Duplicate|\n",
    "|----------|----------|----------|----------|\n",
    "| 1 | 4 | 3 | 0 |\n",
    "| 1 | 3 | 5 | 0 |\n",
    "| 2 | 3 | 4 | 0 |\n",
    "| 2 | 3 | 4 | 1 |\n",
    "| 5 | 8 | 9 | 0 |\n",
    "||||sum=1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates value permanently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.drop` method in `pandas` is used to remove specified labels (rows or columns) from a DataFrame in this case `Class` column removed and other are stored.<br>\n",
    "`axis=1`: Means that you are dropping a column from the DataFrame `data_df`.<br>\n",
    "`y` is used to represent the target variable or the dependent variable.<br>\n",
    "`x` is used to represent the features or independent variables, which are used to predict or explain the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_df.drop('Class',axis=1)\n",
    "Y=data_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data is balanced or not\n",
    "data_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclusion.<br>\n",
    "From the above value_counts of 'Class' Column and from the Graph There are about: '283253' Transactions out of which '473' were Fraud Which means the Data is not properly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge imbalanced-learn --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RandomOverSampler`: This is a specific resampling technique used to address class imbalance. It works by randomly replicating or duplicating instances of the `minority` class (the class with fewer examples) until both classes are balanced.<br>\n",
    "\n",
    "`random_state=1`: This is a parameter provided to the `RandomOverSampler`. It is used to set a random seed for reproducibility. By setting random_state to a specific value (in this case, 1), ensure that the random oversampling process will produce the same results each time we run this code, which is important for reproducible research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply randomoversampler\n",
    "# create object of randomoversampler class\n",
    "ros=RandomOverSampler(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ros.fit_resample(x_train, y_train)`: This line of code applies the Random Over-sampling technique to training data.<br>\n",
    "\n",
    "`x_train` and `y_train` are passed as arguments to the `fit_resample` method of the ros object.<br>\n",
    "\n",
    "`fit_resample`: method examines the class distribution in `y_train` (the target variable) and oversamples the minority class (the class with fewer examples) to balance the class distribution.<br>\n",
    "\n",
    "It returns two sets of data: `x_train_ros` and `y_train_ros`. These sets are now balanced, meaning they have an equal number of examples for each class.<br>\n",
    "\n",
    "After running this code cell, `x_train_ros` will contain the oversampled `feature data`, and `y_train_ros` will contain the corresponding oversampled `labels`. These balanced datasets can then be used to train machine learning models that won't be biased towards the majority class due to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ros,y_train_ros=ros.fit_resample(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ros.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same thing with testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_ros,y_test_ros=ros.fit_resample(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ros.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler` is used for standardizing features by removing the mean and scaling to unit variance.<br>\n",
    "\n",
    "`ss = StandardScaler()`: Create an instance of the `StandardScaler` class.<br>\n",
    "\n",
    "`.fit_transform()`: is a method of the `StandardScaler` object that first calculates the mean and standard deviation of each `feature` in `x_train_ros` and then transforms the data by subtracting the mean and dividing by the standard deviation for each feature. This process standardizes the features.<br>\n",
    "\n",
    "`.transform()`: is used to apply the same mean and standard deviation calculated during the training data standardization to the test data. This ensures that the test data is scaled in the same way as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# ss=StandardScaler()\n",
    "# x_train=ss.fit_transform(x_train_ros)\n",
    "# x_test=ss.transform(x_test_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticReg=LogisticRegression()\n",
    "LogisticReg.fit(x_train_ros,y_train_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=LogisticReg.predict(x_test_ros)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy=(TP+TN)/(TP+TN+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=LogisticReg.score(x_train_ros,y_train_ros)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=metrics.confusion_matrix(y_test_ros,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cm,annot=True,cmap='YlGnBu',xticklabels=['Negative','Positive'],yticklabels=['Negative','Positive'])\n",
    "plt.xlabel('Predicted value')\n",
    "plt.ylabel('Actual value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=pd.read_csv('../new.csv')\n",
    "new_pred=LogisticReg.predict(new_data)\n",
    "new_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
