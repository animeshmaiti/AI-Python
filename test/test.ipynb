{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../Restaurant_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install nltk --yes\n",
    "# !conda install -c conda-forge wordcloud --yes\n",
    "# !conda install -c anaconda scikit-learn --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In Python, `re` stands for \"Regular Expressions.\" It is a built-in module that provides support for regular expressions. Regular expressions are powerful tools for pattern matching and text manipulation. The re module allows you to work with regular expressions in Python, enabling you to search for patterns within strings, replace text based on patterns, and more.\n",
    "\n",
    "#### Here's a brief overview of some commonly used functions and methods in the re module:\n",
    "1. `re.compile(pattern)`: Compiles a regular expression pattern into a regex object for efficient use in matching and searching.\n",
    "1. `re.search(pattern, string)`: Searches for the first occurrence of the pattern in the given string and returns a match object if found.\n",
    "1. `re.match(pattern, string)`: Matches the pattern only at the beginning of the string and returns a match object if it's a match.\n",
    "1. `re.findall(pattern, string)`: Returns all non-overlapping matches of the pattern in the string as a list of strings.\n",
    "1. `re.finditer(pattern, string)`: Returns an iterator yielding match objects for all non-overlapping matches of the pattern in the string.\n",
    "1. `re.sub(pattern, replacement, string)`: Replaces all occurrences of the pattern in the string with the specified replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `nltk.download('stopwords')`: This line is used to download the NLTK stopwords dataset. Stopwords are common words (e.g., \"the,\" \"and,\" \"in\") that are often removed from text during text preprocessing because they don't usually carry significant meaning in text analysis. This download makes sure you have the stopwords dataset available for your NLP tasks.\n",
    "1. `from nltk.corpus import stopwords` : import that stopword dataset\n",
    "1. `from nltk.stem.porter import PorterStemmer`: This import brings in the Porter Stemmer algorithm from NLTK. A stemmer is used in NLP to reduce words to their root or base form. The Porter Stemmer is a popular algorithm for stemming words, which means it removes suffixes from words to convert them to their base form. For example, \"running\" becomes \"run,\" and \"jumps\" becomes \"jump.\" Stemming can be useful for text normalization in various NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk #It is use for Natural language processing\n",
    "import re\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [punc for punc in string.punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `nopunc=[char for char in msg if char not in string.punctuation]`: This line of code is creating a new list called nopunc using a list comprehension. It iterates over each character in the input msg and checks if the character is not in the set of punctuation characters provided by the `string.punctuation` constant. If the character is not a punctuation mark, it's included in the `nopunc` list.\n",
    "1. `nopunc=''.join(nopunc)`: After filtering out punctuation characters, this line joins the characters in the `nopunc` list back together into a single string, effectively removing all punctuation from the original `msg`.\n",
    "1. `nopunc.split()`: This splits the `nopunc` string into a list of words. By default, it splits on whitespace, so it separates the text into individual words.\n",
    "1. `[word for word in nopunc.split() if word.lower() not in stopwords.words('english')]`: This is a list comprehension that further processes the list of words. It iterates over each word in the list, converts it to lowercase using `word.lower()`, and checks if it's not in the list of English `stopwords` obtained from NLTK's `stopwords.words('english')`. If a word is not in the `stopwords` list, it's included in the resulting list.\n",
    "1. `return ' '.join(...)`: Finally, the list of words that are not `stopwords` is joined back together into a single string, with words separated by spaces, and this cleaned and processed text is returned as the output of the `text_process` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this not perfect refine data so it will not work properly\n",
    "# it is doing everything right but it not converting words to there root word basically not using PorterStemmer\n",
    "# def text_process(msg):\n",
    "#     nopunc=[char for char in msg if char not in string.punctuation]\n",
    "#     nopunc=''.join(nopunc)\n",
    "#     return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(msg):\n",
    "    review=re.sub(pattern='[^a-zA-Z]',repl=' ',string=msg)\n",
    "    review=review.lower()\n",
    "    review_words=review.split()\n",
    "    review_words=[word for word in review_words if not word in set(stopwords.words('english'))]\n",
    "    ps=PorterStemmer()\n",
    "    review=[ps.stem(word) for word in review_words]\n",
    "    review=' '.join(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus=[]\n",
    "# for i in range(0,1000):\n",
    "#     review=re.sub(pattern='[^a-zA-Z]',repl=' ',string=data['Review'][i])\n",
    "#     review=review.lower()\n",
    "#     review_words=review.split()\n",
    "#     review_words=[word for word in review_words if not word in set(stopwords.words('english'))]\n",
    "#     ps=PorterStemmer()\n",
    "#     review=[ps.stem(word) for word in review_words]\n",
    "#     review=' '.join(review)\n",
    "#     corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized_review']=data['Review'].apply(text_process)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `word_cloud=data.loc[data['Liked']==1,:]`: This line extracts a subset of data from a DataFrame (assuming data is a DataFrame). It selects rows where the 'Liked' column has a value of 1. This suggests that you are filtering the data to work with a specific subset of text data that is associated with a positive sentiment or some other condition defined by 'Liked' being equal to 1.\n",
    "1. `wordcloud=WordCloud(width=800,height=400,background_color='white').generate(text)`: This code creates a WordCloud object using the WordCloud class, which is presumably imported from the `wordcloud` package. You specify the width, height, and background color for the word cloud. Then, you generate the word cloud from the text you've prepared earlier using the `generate` method.\n",
    "1. `plt.figure(figsize=(10,5))`: It adjusts the size of the output image(10x5 inch).\n",
    "1. `plt.imshow(wordcloud,interpolation='bilinear')`: This line displays the word cloud image using `imshow` from the `matplotlib.pyplot` library. The `interpolation='bilinear'` option specifies how the word cloud image should be interpolated, which affects its visual quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud=data.loc[data['Liked']==1,:]\n",
    "text=' '.join([text for text in word_cloud['Review']])\n",
    "wordcloud=WordCloud(width=800,height=400,background_color='white').generate(text)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "plt.axis('off') # turns off the axis labels \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud=data.loc[data['Liked']==0,:]\n",
    "text=' '.join([text for text in word_cloud['Review']])\n",
    "wordcloud=WordCloud(width=800,height=400,background_color='white').generate(text)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer=CountVectorizer(max_features=1500)\n",
    "vectorizer= CountVectorizer(max_df=0.9,min_df=10)\n",
    "x=vectorizer.fit_transform(data['tokenized_review']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(data['tokenized_review'],data['Liked'],random_state=107,test_size=0.2)\n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorized=vectorizer.transform(X_train)\n",
    "test_vectorized=vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array=train_vectorized.toarray()\n",
    "X_test_array=test_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb=GaussianNB()\n",
    "nb.fit(X_train_array,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds_nb=nb.predict(X_train_array)\n",
    "y_test_preds_nb=nb.predict(X_test_array)\n",
    "# y_test_preds_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"actual_value\":y_test,\"predicted_value\":y_test_preds_nb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,roc_auc_score,confusion_matrix,roc_curve,auc,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(actual,predicted):\n",
    "    print('accuracy score is {}'.format(accuracy_score(actual,predicted)))\n",
    "    print('precision score is {}'.format(precision_score(actual,predicted)))\n",
    "\n",
    "    print('recall score is {}'.format(recall_score(actual,predicted)))\n",
    "    print('f1 score is {}'.format(f1_score(actual,predicted)))\n",
    "    print('roc auc score is {}'.format(roc_auc_score(actual,predicted)))\n",
    "    print('confusion matrix is {}'.format(confusion_matrix(actual,predicted)))\n",
    "    print('classification report is {}'.format(classification_report(actual,predicted)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_train,y_train_preds_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test,y_test_preds_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnv=MultinomialNB()\n",
    "mnv.fit(X_train_array,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds_mnv=mnv.predict(X_train_array)\n",
    "y_test_preds_mnv=mnv.predict(X_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_train,y_train_preds_mnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test,y_test_preds_mnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter tuning\n",
    "best_accuracy=0.0\n",
    "alpha_value=0\n",
    "\n",
    "for i in np.arange(0.01,1.1,0.1):\n",
    "    temp_cls=MultinomialNB(alpha=i)\n",
    "    temp_cls.fit(X_train_array,y_train)\n",
    "    y_test_pred_h_nbayes=temp_cls.predict(X_test_array)\n",
    "    score=accuracy_score(y_test,y_test_pred_h_nbayes)\n",
    "    print(\"accuracy score for alpha-{} is :{}%\".format(round(i,1),round(score*100,2)))\n",
    "    if score>best_accuracy:\n",
    "        best_accuracy=score\n",
    "        alpha_value=i\n",
    "print('.......................')\n",
    "print(\"the best accuracy is {}%\".format(round(best_accuracy*100,2),round(alpha_value,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=MultinomialNB(alpha=0.2)\n",
    "classifier.fit(X_train_array,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sample_review):\n",
    "    sample_review = re.sub(pattern='[^a-zA-Z]',repl=' ',string=sample_review)\n",
    "    sample_review = sample_review.lower()\n",
    "    sample_review_words=sample_review.split()\n",
    "    sample_review_words=[word for word in sample_review_words if not word in set(stopwords.words('english'))]\n",
    "    ps=PorterStemmer()\n",
    "    final_review=[ps.stem(word) for word in sample_review_words]\n",
    "    final_review=' '.join(final_review)\n",
    "    temp=vectorizer.transform([final_review]).toarray()\n",
    "    return classifier.predict(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review=\"food is wonderful pleasing\"\n",
    "if predict_sentiment(review):\n",
    "    print(\"positive review\")\n",
    "else:\n",
    "    print(\"negative review\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
